{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### AIT 614 - Big Data Essentials <br>\n",
    "#### DL2 Team 3 Final Project\n",
    "#### Detecting Abrasive online user content\n",
    "\n",
    "##### Team 3\n",
    "Yasser Parambathkandy\n",
    "\n",
    "Indranil Pal\n",
    "\n",
    "Deepak Rajan\n",
    "\n",
    "<br>\n",
    "\n",
    "##### University\n",
    "George Mason University\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbed6ec8-6c34-4798-8718-6ab57df408ae",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Installation\n",
    "\n",
    "###### Databricks:\n",
    "On an existing cluster, add the following to the Advanced Options -> Spark tab:\n",
    "```\n",
    "  spark.kryoserializer.buffer.max 2000M\n",
    "  spark.serializer org.apache.spark.serializer.KryoSerializer\n",
    "```\n",
    "\n",
    "In Libraries tab inside the cluster:\n",
    "  * Install New -> PyPI -> spark-nlp==4.4.0 -> Install\n",
    "  * Install New -> Maven -> Coordinates -> com.johnsnowlabs.nlp:spark-nlp_2.12:4.4.0 -> Install\n",
    "\n",
    "Refer https://github.com/JohnSnowLabs/spark-nlp#databricks-cluster for installation issues\n",
    "\n",
    "###### Local Machine:\n",
    "See README\n",
    "<br>\n",
    "\n",
    "##### Data load\n",
    "\n",
    "Upload train.csv to databricks. It should get loaded to dbfs:/FileStore/tables/train.csv\n",
    "For local development, update train.csv file path in second cell below\n",
    "<br>\n",
    "##### Saving Model\n",
    "\n",
    "Save model to use in real-time prediction services. The model gets saved to S3 bucket if runtime environment is databricks and AWS accesskey csv has been uploaded to dbfs:/FileStore/tables/ait614_databricks_accessKeys.csv. If either condition is not satisfied, then model is not saved in databricks. To overwrite the file path, bucket name, update properties in second cell below\n",
    "For local development, model is always saved.\n",
    "\n",
    "Saved model is used by prediction services when user enter new questions in UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69fe4605-24f0-4026-9355-ef2f9e09ee20",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sparknlp\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col, udf, lit\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, accuracy_score\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "training file path /home/yarafatin/PycharmProjects/AIT-614-Final/data/train.csv\n"
     ]
    }
   ],
   "source": [
    "#set properties for local machine development and databricks\n",
    "\n",
    "# check if running in Databricks by looking for the DATABRICKS_RUNTIME_VERSION environment variable\n",
    "if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n",
    "    # set the CSV file path for Databricks\n",
    "    train_file_path = 'dbfs:/FileStore/tables/train.csv'\n",
    "    aws_s3_keys = 'dbfs:/FileStore/tables/ait614_databricks_accessKeys.csv'\n",
    "    s3_mount_path = '/mnt/ait614-models'\n",
    "    s3_bucket_name = 'ait614-models'\n",
    "else:\n",
    "    # set the training file path for local machine. this training file should be placed  relative to the current notebook in data directory\n",
    "    train_file_path = '/home/yarafatin/PycharmProjects/AIT-614-Final/data/train.csv'\n",
    "    spark = sparknlp.start()\n",
    "\n",
    "print('training file path {}'.format(train_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feff951e-4086-4180-9307-894502f10189",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- qid: string (nullable = true)\n",
      " |-- question_text: string (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+------+\n",
      "|                 qid|       question_text|target|\n",
      "+--------------------+--------------------+------+\n",
      "|00002165364db923c7e6|How did Quebec na...|     0|\n",
      "|000032939017120e6e44|Do you have an ad...|     0|\n",
      "|0000412ca6e4628ce2cf|Why does velocity...|     0|\n",
      "|000042bf85aa498cd78e|How did Otto von ...|     0|\n",
      "|0000455dfa3e01eae3af|Can I convert mon...|     0|\n",
      "|00004f9a462a357c33be|Is Gaza slowly be...|     0|\n",
      "|00005059a06ee19e11ad|Why does Quora au...|     0|\n",
      "|0000559f875832745e2e|Is it crazy if I ...|     0|\n",
      "|00005bd3426b2d0c8305|Is there such a t...|     0|\n",
      "|00006e6928c5df60eacb|Is it just me or ...|     0|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "schema = \"qid STRING, question_text STRING, target INT\"\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "        .option(\"delimiter\", \",\")\\\n",
    "        .option(\"multiLine\", \"true\")\\\n",
    "        .option(\"quote\", \"\\\"\")\\\n",
    "        .option(\"escape\", \"\\\"\")\\\n",
    "        .schema(schema).csv(train_file_path)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_840B_300 download started this may take some time.\n",
      "Approximate size to download 2.3 GB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "#takes a long time to download, so please be patient\n",
    "embeddings = WordEmbeddingsModel.pretrained(\"glove_840B_300\", \"xx\")\\\n",
    "                .setInputCols(\"document\", \"token\")\\\n",
    "                .setOutputCol(\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler()\\\n",
    "                        .setInputCol(\"question_text\")\\\n",
    "                        .setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer()\\\n",
    "                .setInputCols([\"document\"])\\\n",
    "                .setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nlpPipeline = Pipeline(stages=[document_assembler,\n",
    "                               tokenizer,\n",
    "                               embeddings])\n",
    "\n",
    "df = nlpPipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def avg_vectors(word_vectors):\n",
    "    length = len(word_vectors[0][\"embeddings\"])\n",
    "    avg_vec = [0] * length\n",
    "    for vec in word_vectors:\n",
    "        for i, x in enumerate(vec[\"embeddings\"]):\n",
    "            avg_vec[i] += x\n",
    "        avg_vec[i] = avg_vec[i] / length\n",
    "    return avg_vec\n",
    "\n",
    "\n",
    "def dense_vector(vec):\n",
    "    return Vectors.dense(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 qid|       question_text|target|            document|               token|          embeddings|          doc_vector|\n",
      "+--------------------+--------------------+------+--------------------+--------------------+--------------------+--------------------+\n",
      "|00002165364db923c7e6|How did Quebec na...|     0|[{document, 0, 71...|[{token, 0, 2, Ho...|[{word_embeddings...|[0.38199613126926...|\n",
      "|000032939017120e6e44|Do you have an ad...|     0|[{document, 0, 80...|[{token, 0, 1, Do...|[{word_embeddings...|[-0.7888190010562...|\n",
      "|0000412ca6e4628ce2cf|Why does velocity...|     0|[{document, 0, 66...|[{token, 0, 2, Wh...|[{word_embeddings...|[0.04380194842815...|\n",
      "|000042bf85aa498cd78e|How did Otto von ...|     0|[{document, 0, 56...|[{token, 0, 2, Ho...|[{word_embeddings...|[1.12683903053402...|\n",
      "|0000455dfa3e01eae3af|Can I convert mon...|     0|[{document, 0, 76...|[{token, 0, 2, Ca...|[{word_embeddings...|[3.57917102612555...|\n",
      "|00004f9a462a357c33be|Is Gaza slowly be...|     0|[{document, 0, 71...|[{token, 0, 1, Is...|[{word_embeddings...|[0.19421800225973...|\n",
      "|00005059a06ee19e11ad|Why does Quora au...|     0|[{document, 0, 11...|[{token, 0, 2, Wh...|[{word_embeddings...|[-1.2030513910576...|\n",
      "|0000559f875832745e2e|Is it crazy if I ...|     0|[{document, 0, 68...|[{token, 0, 1, Is...|[{word_embeddings...|[-1.8848890915978...|\n",
      "|00005bd3426b2d0c8305|Is there such a t...|     0|[{document, 0, 10...|[{token, 0, 1, Is...|[{word_embeddings...|[-1.2872349936515...|\n",
      "|00006e6928c5df60eacb|Is it just me or ...|     0|[{document, 0, 24...|[{token, 0, 1, Is...|[{word_embeddings...|[-1.1119125646073...|\n",
      "+--------------------+--------------------+------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# create a udf\n",
    "avg_vectors_udf = udf(avg_vectors, ArrayType(DoubleType()))\n",
    "df_doc_vec = df.withColumn(\"doc_vector\", avg_vectors_udf(col(\"embeddings\")))\n",
    "df_doc_vec.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb56a788-7f06-49c0-96fb-3e4a54921d45",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dense_vector_udf = udf(dense_vector, VectorUDT())\n",
    "word_df = df_doc_vec.withColumn(\"features\", dense_vector_udf(col(\"doc_vector\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8af8dc69-5e1e-4b39-b72c-853ed6123eb7",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# split train and test data\n",
    "# code inspired from https://stackoverflow.com/questions/47637760/stratified-sampling-with-pyspark\n",
    "split_ratio = 0.8\n",
    "seed = 42\n",
    "fractions = word_df.select('target').distinct().withColumn(\"fraction\", lit(split_ratio)).filter(\n",
    "    'fraction is not null').rdd.collectAsMap()\n",
    "\n",
    "train_df = word_df.stat.sampleBy('target', fractions, seed)\n",
    "test_df = word_df.join(train_df, on='qid', how=\"left_anti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33ece78c-da00-49bc-a605-310cc1797319",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"target\", featuresCol=\"features\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test_df)\n",
    "predictions.select(\"target\", \"prediction\").show(n=10, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model performance evaluations\n",
    "pred_df = predictions.select('target', 'prediction').toPandas()\n",
    "print(classification_report(pred_df.target, pred_df.prediction))\n",
    "print(accuracy_score(pred_df.target, pred_df.prediction))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Compute AUC-ROC\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"target\", rawPredictionCol=\"prediction\",\n",
    "                                                 metricName=\"areaUnderROC\")\n",
    "auc_roc = binary_evaluator.evaluate(predictions)\n",
    "print(\"AUC-ROC: {:.2f}%\".format(auc_roc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d920ac0a-ec37-4b1e-a2e9-f8ac8a5f268e",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Compute AUC-PR\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"target\", rawPredictionCol=\"prediction\",\n",
    "                                                 metricName=\"areaUnderPR\")\n",
    "auc_pr = binary_evaluator.evaluate(predictions)\n",
    "print(\"AUC-PR: {:.2f}%\".format(auc_pr * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ebe5dc8-56cb-4078-a624-99de7887d924",
     "showTitle": false,
     "title": ""
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "results = predictions.select(['probability', 'target']).collect()\n",
    "results_list = [(float(i[0][1]), 1.0 - float(i[1])) for i in results]\n",
    "fpr, tpr, _ = roc_curve([i[1] for i in results_list], [i[0] for i in results_list])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save model. if running in databricks save to s3.\n",
    "model_save_dir = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\") + '-word-model'\n",
    "\n",
    "if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n",
    "    try:\n",
    "        dbutils.fs.ls(aws_s3_keys)\n",
    "        aws_keys_df = spark.read.csv(aws_s3_keys)\n",
    "\n",
    "        ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "        SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "        ENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n",
    "        # see https://docs.databricks.com/dbfs/mounts.html#mount-a-bucket-using-aws-keys for reference\n",
    "        dbutils.fs.mount(f\"s3a://{ACCESS_KEY}:{ENCODED_SECRET_KEY}@{s3_bucket_name}\", s3_mount_path)\n",
    "        lrModel.save(s3_mount_path + '/' + model_save_dir)\n",
    "    except Exception as e:\n",
    "        print('no aws access key file loaded, so model is not being saved to s3')\n",
    "else:\n",
    "    lrModel.save(model_save_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "word_embedding_model_spark_nb",
   "notebookOrigID": 747062604701134,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}